== Deploying an Apache Kafka cluster with Strimzi
ifdef::env-github,env-browser[:outfilesuffix: .adoc]
:imagesdir: ./images

Let's get started by deploying an Apache Kafka cluster on OpenShift.
For this, we will use http://strimzi.io/[Strimzi], an open-source project that simplifies the process of deploying and managing Apache Kafka clusters on Kubernetes and OpenShift.

=== How Strimzi works

You can run an Apache Kafka cluster on Kubernetes, and by extension, on OpenShift, in a variety of ways, not all being equal in terms of ease of use and maintenance.

For example, you can deploy the cluster manually as a stateful set.
While this can get you past the initial hurdle of starting the cluster, soon you have to start performing more complex tasks such as changing cluster topology, modifying configuration, or administering topics.
These tasks typically require direct access to the cluster nodes and can easily become cumbersome.

==== Kubernetes Operators ====

Strimzi simplifies these tasks by using a declarative approach to cluster and topic management, implemented as https://coreos.com/operators/[Kubernetes Operators].
A Kubernetes Operator is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user.
It builds upon the basic Kubernetes resource and operator concepts but includes domain or application-specific knowledge to automate common tasks.

Instead of relying on direct deployment and management of Zookeeper and Kafka clusters, Strimzi consists of a couple of these domain-specific operators that monitor the state of the cluster, making adjustments in accordance to a desired state read from dedicated configuration resources.

For creating an Apache Kafka cluster, for instance, you need to create a resource of type `kafka` that describes the properties of the cluster, and the *_cluster operator_* will deploy the cluster for you.
If you need to change the state of the cluster, for example for changing properties or for adding new instances, all you have to do is to modify the resource and the changes will be rolled out accordingly.

Topic management works in a similar fashion: for creating and modifying topics, you only need to create and edit a set of resources of type `kafkatopic` and the *_topic operator_* will do the work for you.

You will do all this as part of the first lab.

=== Connecting to Your VM

First, connect via SSH to the Vagrant VM on your machine or your remote cloud environment.
For a local Vagrant box run:

[source, sh]
$ vagrant up
$ vagrant ssh

[NOTE]
====
If you have the Vagrant vbguest plug-in installed, you may run into this error:

[source, sh]
----
==> default: Checking for guest additions in VM...
The following SSH command responded with a non-zero exit status.
Vagrant assumes that this means the command failed!

 setup

Stdout from the command:

Stderr from the command:

bash: line 4: setup: command not found
----

Try running the following in this case:

[source, sh]
$ vagrant vbguest --do install
====

Otherwise, run:

[source, sh]
$ ssh <YOUR SERVER IP>

=== Starting OpenShift

Switch to the `build` user:

[source, sh]
$ sudo su - build

Start up OpenShift (replace the IP address with your actual value when not using Vagrant or using Vagrant with a different IP):

[source, sh]
$ oc cluster up --routing-suffix=192.168.33.10.nip.io

This make take a few minutes.

Once OpenShift has started, log in:

[source, sh]
$ oc login -u developer

==== Environment cleanup

[NOTE]
====
Don't execute these steps now, but only at the end of the lab.
====

After you have completed all modules in this workshop then you can clean up the environment by issuing these commands:

[source,sh]
$ oc cluster down
$ mount | grep -o '/home/build/openshift.local.clusterup/[^ ]*' | xargs sudo umount; sudo rm -rf $HOME/openshift.local.clusterup

=== Installing Strimzi

Download Strimzi and extract it;

[source, sh]
$ wget https://github.com/strimzi/strimzi/releases/download/0.8.0/strimzi-0.8.0.tar.gz
$ tar xzvf strimzi-0.8.0.tar.gz

Enter the `strimzi-0.8.0` directory.

[source, sh]
$ cd strimzi-0.8.0

https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/[Namespaces] (or "projects" in OpenShift terms) are a concept of Kubernetes for organizing and dividing the resources of a cluster between multiple users.
For the lab we'll create a project named "voxxed":

[source, sh]
$ oc new-project voxxed

We must adjust the namespace in the Strimzi installation files to match that project name,
allowing to install Strimzi's resources into that project.
The `sed` tool can be used to do so:

[source, sh]
$ sed -i 's/namespace: .*/namespace: voxxed/' install/cluster-operator/*RoleBinding*.yaml

You can see that the project is empty:

[source, sh]
$ oc get pods
No resources found.

Log in as administrator to install Strimzi:

[source,sh]
$ oc login -u system:admin

Install the cluster operator:

[source, sh]
$ oc apply -f install/cluster-operator -n voxxed

You should see a few resources being created:

[source, sh]
serviceaccount "strimzi-cluster-operator" created
clusterrole.rbac.authorization.k8s.io "strimzi-cluster-operator-namespaced" created
rolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator" created
clusterrole.rbac.authorization.k8s.io "strimzi-cluster-operator-global" created
clusterrolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator" created
clusterrole.rbac.authorization.k8s.io "strimzi-kafka-broker" created
clusterrolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator-kafka-broker-delegation" created
clusterrole.rbac.authorization.k8s.io "strimzi-entity-operator" created
rolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator-entity-operator-delegation" created
clusterrole.rbac.authorization.k8s.io "strimzi-topic-operator" created
rolebinding.rbac.authorization.k8s.io "strimzi-cluster-operator-topic-operator-delegation" created
customresourcedefinition.apiextensions.k8s.io "kafkas.kafka.strimzi.io" created
customresourcedefinition.apiextensions.k8s.io "kafkaconnects.kafka.strimzi.io" created
customresourcedefinition.apiextensions.k8s.io "kafkaconnects2is.kafka.strimzi.io" created
customresourcedefinition.apiextensions.k8s.io "kafkatopics.kafka.strimzi.io" created
customresourcedefinition.apiextensions.k8s.io "kafkausers.kafka.strimzi.io" created
deployment.extensions "strimzi-cluster-operator" created

The service account `strimzi-cluster-operator` is granted permission to access various resources in the project.
This allows it to read the resources containing the cluster configuration that we will create later in the process.

Now, make sure that the cluster operator is deployed.

[source,sh]
$ oc get pods

The command output should be similar to:

[source,sh]
NAME                                          READY     STATUS    RESTARTS   AGE
strimzi-cluster-operator-2044197322-lzrvr   1/1       Running   0          3m

You also can log into the OpenShift web console to do the same.
Go to https://192.168.33.10:8443 (adjust IP if needed; accept all warnings about certificate issues),
log in using "developer" and any password, and choose the "voxxed" project.
You should see a single deployed application, "strimzi-cluster-operator".
All the applications you'll deploy in the following will show up automatically, too.

Next, install the Strimzi templates.
The Cluster Operator related templates contain predefined resources for easily deploying clusters (for Kafka Connect as well).

[source, sh]
$ oc apply -f examples/templates/cluster-operator -n voxxed
template.template.openshift.io "strimzi-connect-s2i" created
template.template.openshift.io "strimzi-connect" created
template.template.openshift.io "strimzi-ephemeral" created
template.template.openshift.io "strimzi-persistent" created

Now you can deploy a Kafka cluster.
For this lab, we will use a template (so to bootstrap only a single ZooKeeper node),
but you could create a resource file from scratch as well or use one from _examples/kafka_.
We will deploy 3 instances of Kafka broker (the default) and one instance of ZooKeeper by applying the "strimzi-persistent" template.

The deployed Kafka broker instances and the ZooKeeper are by default configured to use all available memory provided by the environment.
This is usually undesirable so we will set OpenShift limits to them so each of the pods will receive only allocated portion of memory which is 1 GB in case of each broker instance and 512 MB in case of ZooKeeper.
The CPU pressure is not so important in our case so we will keep it unlimited.
This is done via `oc patch` the newly set up cluster resource.
Here are the two commands:

[source, sh]
$ oc process strimzi-persistent \
    -p ZOOKEEPER_NODE_COUNT=1 \
    -p KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    -p KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=1 \
    | oc apply -f - && \
    oc patch kafka my-cluster --type merge -p '{ "spec" : { "zookeeper" : { "resources" : { "limits" : { "memory" : "512Mi" }, "requests" : { "memory" : "512Mi" } } },  "kafka" : { "resources" : { "limits" : { "memory" : "1Gi" }, "requests" : { "memory" : "1Gi" } } } } }'

kafka "my-cluster" created

Let's take a look at the resource we've created:

[source, sh]
$ oc describe kafka my-cluster
Name:         my-cluster
Namespace:    voxxed
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"kafka.strimzi.io/v1alpha1","kind":"Kafka","metadata":{"annotations":{},"name":"my-cluster","namespace":"voxxed"},"spec":{"entityOperator...
API Version:  kafka.strimzi.io/v1alpha1
Kind:         Kafka
Metadata:
  Creation Timestamp:  2018-10-25T16:44:07Z
  Generation:          1
  Resource Version:    2746
  Self Link:           /apis/kafka.strimzi.io/v1alpha1/namespaces/voxxed/kafkas/my-cluster
  UID:                 30e91dbe-d875-11e8-9a35-96000012a06f
Spec:
  Entity Operator:
    Topic Operator:
    User Operator:
  Kafka:
    Config:
      Default . Replication . Factor:                    1
      Offsets . Topic . Replication . Factor:            1
      Transaction . State . Log . Replication . Factor:  1
    Listeners:
      Plain:
      Tls:
    Liveness Probe:
      Initial Delay Seconds:  15
      Timeout Seconds:        5
    Readiness Probe:
      Initial Delay Seconds:  15
      Timeout Seconds:        5
    Replicas:                 3
    Storage:
      Type:  ephemeral
  Zookeeper:
    Liveness Probe:
      Initial Delay Seconds:  15
      Timeout Seconds:        5
    Readiness Probe:
      Initial Delay Seconds:  15
      Timeout Seconds:        5
    Replicas:                 1
    Storage:
      Type:  ephemeral
Events:      <none>

Note how for instance the number of Kafka and ZooKeeper nodes is controlled using the `Replicas` parameters.

Visualize the running pods:

[source,sh]
$ oc get pods -w

Wait until all pods have spun up and are in `Running` status:

[source,sh]
$ oc get pods -w
NAME                                          READY     STATUS    RESTARTS   AGE
my-cluster-entity-operator-8669d89df6-g975b   3/3       Running   0          3m
my-cluster-kafka-0                            2/2       Running   0          4m
my-cluster-kafka-1                            2/2       Running   0          4m
my-cluster-kafka-2                            2/2       Running   0          4m
my-cluster-zookeeper-0                        2/2       Running   0          4m
strimzi-cluster-operator-7d8898b9b9-jfwv5     1/1       Running   0          14m

In addition to the `cluster operator` created previously, notice a few more deployments:

* the `entity operator` is now deployed as well - you can deploy it independently, but the Strimzi template deploys it out of the box; it is used to manage topics and/or users of Kafka
* one Zookeeper node
* three Kafka brokers

Also, notice that the Zookeeper ensemble and the Kafka cluster are deployed as stateful sets.

=== Monitoring with Prometheus and Grafana

By default, Strimzi provides the Kafka brokers and the Zookeeper nodes with a Prometheus JMX exporter agent which is running in order to export metrics.
These metrics can be read and processed by a Prometheus server in order to monitoring the cluster.
For building a graphical dashboard with such information, it's possible to use Grafana.

==== Prometheus

The Prometheus service pod runs with `prometheus-server` service account and it needs to have access to the API server to get the pod list and for allowing that, the following command is needed.

[source,sh]
$ export NAMESPACE=voxxed
$ oc create sa prometheus-server
$ oc adm policy add-cluster-role-to-user cluster-reader system:serviceaccount:${NAMESPACE}:prometheus-server

Create the Prometheus service by running:

[source,sh]
$ oc apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.8.0/metrics/examples/prometheus/kubernetes.yaml

Deploy Node Exporter to collect system-level metrics:

[source,sh]
$ oc adm policy add-cluster-role-to-user prometheus-server system:serviceaccount:${NAMESPACE}:prometheus-server
$ oc adm policy add-scc-to-user -z prometheus-node-exporter hostnetwork
$ oc adm policy add-scc-to-user -z prometheus-node-exporter hostaccess
$ oc apply -f https://raw.githubusercontent.com/openshift/origin/v3.11.0/examples/prometheus/node-exporter.yaml

Finally it is necessary to enable metrics collection from Kafka brokers by defining a set of data available from JMX.
This is configured in the Kafka resource.

[source,sh]
----
# Download Kafka metrics definition file
$ wget https://raw.githubusercontent.com/debezium/microservices-lab/master/vm-setup/kafka-metrics.yaml

# Apply the metrics to the kafka resource
$ oc get kafka my-cluster -o yaml | sed '/kafka:/ r kafka-metrics.yaml' - | oc apply -f -
----

After that the nodes in the cluster are automatically restarted.

==== Grafana

The Grafana server is really useful to get a visualisation of the Prometheus metrics.

To deploy Grafana on OpenShift, the following commands should be executed:

[source,sh]
$ oc apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.8.0/metrics/examples/grafana/kubernetes.yaml

You can access the Grafana UI after running:

[source,sh]
$ oc expose svc/grafana

The hostname of the service is available in the OpenShift console, or can be retrieved via CLI:

[source,sh]
$ oc get routes grafana -o=jsonpath='{.spec.host}{"\n"}'

Note the output, which should be in the format `grafana-voxxed.<YOUR IP>.nip.io` and access the Grafana UI at that URL in your browser.
Now we can set up the Prometheus data source and the Kafka dashboard.

Access to the Grafana UI using `admin/admin` credentials
(you'll be required to change the password after logging in).

image::grafana_login.png[grafana login]

Click on the "Add data source" button from the Grafana home in order to add Prometheus as data source.

image::grafana_home.png[grafana home]

Fill in the information about the Prometheus data source, specifying a name and "Prometheus" as type.
In the URL field, use `http://prometheus:9090` as the URL to the Prometheus server.
After "Add" is clicked, Grafana will test the connection to the data source.

image::grafana_prometheus_data_source.png[grafana prometheus data source]

From the top left menu, click on "Dashboards" and then "Import" to open the "Import Dashboard" window.
Open a browser tab and navigate to `https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.8.0/metrics/examples/grafana/strimzi-kafka.json`.
You should see JSON content as response.
Copy and paste it in the appropriate field in the form.

image::grafana_import_dashboard.png[grafana import dashboard]

After importing the dashboard, the Grafana home should show with some initial metrics about CPU and JVM memory usage.
When the Kafka cluster is used (creating topics and exchanging messages) the other metrics, like messages in and bytes in/out per topic, will be shown.

image::grafana_kafka_dashboard.png[grafana kafka dashboard]

If you are interested you can create a dashboard also for Kafka Connect instance that will be used later in this exercise.
The dashboard definition can be found at `https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.8.0/metrics/examples/grafana/strimzi-kafka-connect.json`.

=== Handling cluster and topics

Before starting to develop data streaming applications and running them, let's see how it's possible to handle the Kafka cluster itself and the topics.

==== Updating Kafka cluster

Starting from the current Kafka cluster with 3 brokers, we want to scale down to one as it is sufficient in development environment.
In order to do that, the related `my-cluster` resource needs to be updated using the "edit" command provided by the `oc` tool.

[source,sh]

$ oc edit kafka my-cluster

It opens the default editor that we can use in order to change the value of the `spec/kafka/replicas` field from 3 to 1 (quite toward the end of the `kafka` section).
After saving the file, the Cluster Operator detects the update and removes two broker pods; it's just a simple scale-down operation.
You can see this by visualizing the pods again:

[source,sh]
$ oc get pods
NAME                                          READY     STATUS    RESTARTS   AGE
my-cluster-entity-operator-549b687c88-gb4w9   3/3       Running   0          45m
my-cluster-kafka-0                            2/2       Running   0          46m
my-cluster-zookeeper-0                        2/2       Running   0          38s
strimzi-cluster-operator-5bbcc486fc-hwswp     1/1       Running   0          47m

Notice the pods `my-cluster-kafka-1` and `my-cluster-kafka-2` are first in state "Terminating" and should then disappear after a while.
For the rest of the lab, we recommend keep the size to one node.

==== Handling topics

It's possible to create a topic by creating a `kafkatopic` resource from scratch, but for this lab we are going to use the related example resource file:

[source,sh]
$ oc apply -f examples/topic/kafka-topic.yaml
kafkatopic "my-topic" created

In order to check that the Topic Operator has detected the new resource and created a related topic in the Kafka cluster, we can run the official `kafka-topics.sh` tool on one of the brokers.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -c kafka -- bin/kafka-topics.sh --zookeeper localhost:2181 --describe
Topic:my-topic	PartitionCount:1	ReplicationFactor:1	Configs:segment.bytes=1073741824,retention.ms=7200000
	Topic: my-topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0

You also can examine the topic resource itself using `oc describe`:

[source,sh]
$ oc describe kafkatopic my-topic
Name:         my-topic
Namespace:    voxxed
Labels:       strimzi.io/cluster=my-cluster
Annotations:  kubectl.kubernetes.io/last-applied-configuration={"apiVersion":"kafka.strimzi.io/v1alpha1","kind":"KafkaTopic","metadata":{"annotations":{},"labels":{"strimzi.io/cluster":"my-cluster"},"name":"my-topi...
API Version:  kafka.strimzi.io/v1alpha1
Kind:         KafkaTopic
Metadata:
  Cluster Name:
  Creation Timestamp:  2018-10-09T14:48:53Z
  Resource Version:    2010
  Self Link:           /apis/kafka.strimzi.io/v1alpha1/namespaces/voxxed/kafkatopics/my-topic
  UID:                 7136321b-cbd2-11e8-b135-96000011cbea
Spec:
  Config:
    Retention . Ms:   7200000
    Segment . Bytes:  1073741824
  Partitions:         1
  Replicas:           1
Events:               <none>

[NOTE]
====
If you don't see the topic, or the last command yields an event stating "Failure processing KafkaTopic watch event ADDED",
then the topic operator may have lost the connection to the ZooKeeper node.
That's a known issue in the operator which should be fixed soon.
For now you can work around this by deleting the pod that runs the topic operator.
OpenShift will automatically start a new instance:

[source,sh]
$ oc delete $(oc get pods -o name -l strimzi.io/name=my-cluster-entity-operator)
====

Let's increase the partitions number now:

[source,sh]
$ oc edit kafkatopic my-topic

Set the value of `spec/partitions` to `3`.

Alternatively, you could also edit the file _examples/topic/kafka-topic.yaml_ (e.g. using _vi_) and apply it again;

[source,sh]
$ oc apply -f examples/topic/kafka-topic.yaml
kafkatopic "my-topic" configured

The Topic Operator updates the related Kafka topic accordingly.
We can check that describing the topic one more time.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -c kafka -- bin/kafka-topics.sh --zookeeper localhost:2181 --describe
Topic:my-topic	PartitionCount:3	ReplicationFactor:1	Configs:segment.bytes=1073741824,retention.ms=7200000
	Topic: my-topic	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
	Topic: my-topic	Partition: 1	Leader: 0	Replicas: 0	Isr: 0
	Topic: my-topic	Partition: 2	Leader: 0	Replicas: 0	Isr: 0

Finally, a topic can be deleted like so:

[source,sh]
$ oc delete kafkatopic my-topic
kafkatopic "my-topic" deleted

The Topic Operator detects the deletion of the resource and deletes the related Kafka topic from the cluster.
We can check that listing the available topics.

[source,sh]
$ oc exec -it my-cluster-kafka-0 -c kafka -- bin/kafka-topics.sh --zookeeper localhost:2181 --list

This time the output should be empty.

Finally, let's switch from the admin account to the developer one:

[source,sh]
$ oc login -u developer

Now that your Kafka cluster is running and ready to go, let's continue with <<module-02#,module 2>> and build some applications!
